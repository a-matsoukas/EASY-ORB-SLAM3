---
layout: post
title: Milestone 1
permalink: /milestone1
---

## Project and Milestone 1 Goals

Our project goal is to use a video feed from a flying drone to create a 3-dimensional map of the drone’s surrounding environment and locate the drone within that map. As a stretch goal, we are planning to give the drone a starting marker and a stopping marker, have it locate them while making the map, and create an optimal path from start to stop, given the map it has created. The drone we are using is a DJI Mini, and we will be using an algorithm called visual simultaneous localization and mapping (vSLAM).

By our first milestone on November 16, 2023, we had planned in our project proposal to have a drone that we could teleoperate and receive data from, including video, current altitude, and odometry.

## Milestone 1 Update: 1.5 Week Progress

We spent our first in-class work time understanding the software tools needed to control our drone with a Python script. Because DJI drones use a proprietary communication language between them and their controllers, we need to use a software called [RosettaDrone 2](https://github.com/RosettaDrone/rosettadrone), which creates a wrapper around the DJI SDK that translates MAVLink commands from a Ground Station into the drones proprietary language; [MAVLink](https://mavlink.io/en/) is a standard, lightweight communication protocol for communicating with drones. A Python script generating these MAVLink commands can be linked to the RosettaDrone 2 app.

In order to maximize our development time, and because we only have one drone, we divided our work into 2 sections to progress simultaneously: Alex researched how vSLAM works, and Rajiv set up the suite of software tools necessary for DJI drone communication.

### External Tool Setup and Drone Software

The DJI drones are typically controlled through a wireless transmitter/controller which relays a video feed through an android or IOS device wired into the transmitter using the DJI provided app. To programmatically control drone movement and receive data, we needed to set up and load onto an android device a 3rd party controller app (RosettaDrone) to mimic the DJI proprietary controller. By using RosettaDrone, it can connect to 3rd party “Ground Stations” which can send commands through a connection to RosettaDrone on the android device which then travels through the transmitter to the drone.

In terms of setup, RosettaDrone required 2 large steps for individuals unfamiliar with Android development and SDK’s in general: generating API keys for the DJI mobile SDK and Google Maps, and loading the built rosettaDrone app onto an Android device.

QGroundControl is another open source tool that is being used as our first “Ground Station” for testing programmatic drone control features. It allows camera access, sending movement commands, accessing sensor data, and can be used alongside other Ground Stations or manual control simultaneously. It is a very useful debugging tool. Unlike RosettaDrone, this lives on the computer and connects to the RosettaDrone instance over wifi.

So, where did we get? Well, we have the API keys generated for Google Maps and DJI, and have built the application and loaded it onto an android device. We did not have time to test programmatic running of the drone, but all of our tools seem to be playing nicely so far!

The future: Our end goal is to use DroneKit as our ground station to send python commands and receive data from the Drone. Once we have tested QGroundControl’s (QGC) functionality, we will begin using DroneKit in conjunction with QGC to aid in debugging before phasing out QGC in favor of only programmatic access. We will also be connecting our DroneKit scripts to openCV, which is supported natively.

### SLAM Research

In general, SLAM algorithms have two parts: a front-end part that relies on a sensor such as a camera or LiDAR to estimate the location of obstacles and a back-end part that processes the sensor data to identify previously-seen landmarks, localize the robot, and generate a map. By continuously exploring, remembering landmarks, and determining how far the robot is from each of the known landmarks, the algorithm generates a map and can place the robot within the map simultaneously. When a single camera is used as the only sensor, known as monocular SLAM, it becomes challenging to determine the depth of objects in the frame; this can be mitigated by detecting known patterns in the image such as checkerboards or by combining camera data with data from an inertial measurement unit (IMU). The next step after this high-level research is to look into implementing the back-end portion of vSLAM or make use of an existing implementation; additionally, this portion of the project can be accomplished without having an operable drone, allowing us to work in parallel without getting hung up on hardware communication.
