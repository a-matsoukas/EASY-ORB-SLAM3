---
layout: post
title: Milestone 2
permalink: /milestone2
---

## Project and Milestone 2 Goals

Our project goal is to use a video feed from a flying drone to create a 3-dimensional map of the drone’s surrounding environment and locate the drone within that map. As a stretch goal, we are planning to give the drone a starting marker and a stopping marker, have it locate them while making the map, and create an optimal path from start to stop, given the map it has created. The drone we are using is a DJI Mini, and we will be using an algorithm called visual simultaneous localization and mapping (vSLAM).

By our second milestone on December 5, 2023, we had planned in our project proposal that we would be able to teleoperate a drone to collect video data to be used for mapping, and that we would have a first pass on a vSLAM algorithm working.

## Milestone 2 Update: 3 Week Update

Our project is currently split into two parts, which we intend to integrate together for our final product. The first is working with the DJI Mini setup and allowing it to be controlled via Python script. The second is working on development of the monocular vSLAM algorithm. The final product will be to run vSLAM on the video feed that is received from the drone. These pieces do not depend on each other, so we can continue development in parallel.

### External Tool Setup and Drone Software

We were successful in installing and running RosettaDrone on an android device and connecting to the drone. From the user interface provided by the RosettaDrone software, we are able to receive camera, speed, and altitude data from the drone; however, we are still working on sending commands to the drone to control it.

Our next steps in setting up the tools and drone to be ready for use as intended for our project will be to first investigate how to use RosettaDrone to manually send commands to the drone through the controller. After that, we will look into integrating ground control software (QGroundControl), which is run from a computer and connects to RosettaDrone over the internet. The final step will be to send commands to RosettaDrone from a Python script, as RosettaDrone can be connected to both QGroundControl and receive commands from a Python script at the same time.

### SLAM Implementation

After conducting research on the steps involved in vSLAM, we have determined that the overall vSLAM implementation can be split into three steps:

- Visual odometry, in which a map of the drone’s surroundings is constructed based on the view from its camera
- Loop closure, in which previously collected data is refined to account for new input data, if the drone encounters an area it has previously flown through
- Localization, in which the position of the drone within the generated map is determined based on what it sees in the camera.

From our initial research phase, we have discovered a [YouTube tutorial and code implementation of visual odometry](https://www.youtube.com/watch?v=N451VeA8XRA), and we have found a [GitHub repository that runs monocular ORB-SLAM3](https://github.com/arthurfenderbucker/indoor_drone) on a drone camera to map indoor environments. This repository uses a .rosinstall file to integrate external ROS packages, which run Drone Control, VSLAM, and visualizations.

As a first pass at visual odometry, we started with the code from the YouTube tutorial. After looking through the code in the linked GitHub repository, we were struck by the way in which its ROS implementation allowed for the integration of discrete pieces of code. We are using that code as a starting point; as a first step we will need to learn how to modify the .rosinstall file to switch out existing packages and add in our own packages specific to the DJI drone. Our reach goal is to fully replace the vSLAM implementation with our own and compare its results with the implementation that already exists.
